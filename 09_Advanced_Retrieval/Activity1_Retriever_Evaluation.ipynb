{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity #1: Retriever Evaluation with RAGAS\n",
    "\n",
    "This notebook evaluates different retriever methods using RAGAS for synthetic dataset generation.\n",
    "\n",
    "### Objectives:\n",
    "1. Create a \"golden dataset\" using RAGAS Synthetic Data Generation\n",
    "2. Evaluate 6 different retrievers on combined CSV + PDF data\n",
    "3. Compare performance, cost, and latency\n",
    "4. Provide recommendations\n",
    "\n",
    "### Data Sources:\n",
    "- **CSV Data**: Consumer complaint narratives\n",
    "- **PDF Data**: Federal Student Aid handbooks\n",
    "\n",
    "### Retrievers to Evaluate:\n",
    "- Naive Retrieval (Embedding-based)\n",
    "- BM25 Retriever\n",
    "- Multi-Query Retriever\n",
    "- Parent-Document Retriever\n",
    "- Contextual Compression (Reranking)\n",
    "- Ensemble Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LangSmith tracing enabled\n",
      "‚úÖ API keys configured\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import getpass\n",
    "\n",
    "# Set up API keys\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")\n",
    "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Enter your Cohere API Key:\")\n",
    "\n",
    "# Optional: Set up LangSmith for advanced evaluation\n",
    "try:\n",
    "    os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API Key (optional, press Enter to skip):\")\n",
    "    if os.environ[\"LANGCHAIN_API_KEY\"]:\n",
    "        os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "        os.environ[\"LANGCHAIN_PROJECT\"] = \"Retriever-Evaluation\"\n",
    "        print(\"‚úÖ LangSmith tracing enabled\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  LangSmith skipped\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è  LangSmith skipped\")\n",
    "\n",
    "print(\"‚úÖ API keys configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 825 complaint documents from CSV\n",
      "‚úÖ Loaded 269 PDF documents\n",
      "‚úÖ Total documents: 1094 (CSV: 825, PDF: 269)\n",
      "\n",
      "Sample complaint: The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans current...\n",
      "Sample PDF content: Volume 3\n",
      "Academic Calendars, Cost of Attendance, and\n",
      "Packaging\n",
      "Introduction\n",
      "This volume of the Federal Student Aid (FSA) Handbook discusses the academ...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# Load CSV data\n",
    "loader = CSVLoader(\n",
    "    file_path=\"./data/complaints.csv\",\n",
    "    metadata_columns=[\n",
    "        \"Date received\", \"Product\", \"Sub-product\", \"Issue\", \"Sub-issue\", \n",
    "        \"Consumer complaint narrative\", \"Company\", \"State\", \"Complaint ID\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "loan_complaint_data = loader.load()\n",
    "\n",
    "# Set page content to complaint narrative\n",
    "for doc in loan_complaint_data:\n",
    "    doc.page_content = doc.metadata[\"Consumer complaint narrative\"]\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(loan_complaint_data)} complaint documents from CSV\")\n",
    "\n",
    "# Load PDF data\n",
    "path = \"data/\"\n",
    "pdf_loader = DirectoryLoader(path, glob=\"*.pdf\", loader_cls=PyMuPDFLoader)\n",
    "pdf_docs = pdf_loader.load()\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(pdf_docs)} PDF documents\")\n",
    "\n",
    "# Combine all documents\n",
    "all_docs = loan_complaint_data + pdf_docs\n",
    "print(f\"‚úÖ Total documents: {len(all_docs)} (CSV: {len(loan_complaint_data)}, PDF: {len(pdf_docs)})\")\n",
    "\n",
    "print(f\"\\nSample complaint: {loan_complaint_data[0].page_content[:150]}...\")\n",
    "print(f\"Sample PDF content: {pdf_docs[0].page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Golden Dataset using RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAGAS models initialized\n"
     ]
    }
   ],
   "source": [
    "# RAGAS setup\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# Initialize models for RAGAS\n",
    "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
    "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n",
    "\n",
    "print(\"‚úÖ RAGAS models initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic test dataset using RAGAS...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7da9087841b14137bdb198015529c6f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying HeadlinesExtractor:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5343091113e84a33a4153bd7bbad2ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying HeadlineSplitter:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06b94dda1f544487842bfe0a1bc73004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying SummaryExtractor:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Property 'summary' already exists in node '14cdd6'. Skipping!\n",
      "Property 'summary' already exists in node '11fcb0'. Skipping!\n",
      "Property 'summary' already exists in node '86bf6b'. Skipping!\n",
      "Property 'summary' already exists in node '61dc5f'. Skipping!\n",
      "Property 'summary' already exists in node '69767f'. Skipping!\n",
      "Property 'summary' already exists in node '9d7765'. Skipping!\n",
      "Property 'summary' already exists in node '83f2b3'. Skipping!\n",
      "Property 'summary' already exists in node 'c25e73'. Skipping!\n",
      "Property 'summary' already exists in node '5b50f1'. Skipping!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c990c0ed7b841fea38a821939be643b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying CustomNodeFilter:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40f02d7644d3456ba10d3789bd44bc69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Property 'summary_embedding' already exists in node '61dc5f'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '11fcb0'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '14cdd6'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '5b50f1'. Skipping!\n",
      "Property 'summary_embedding' already exists in node 'c25e73'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '83f2b3'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '9d7765'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '86bf6b'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '69767f'. Skipping!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13fcb2e1adae4101b7d1c8585b40cf77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dbd2ebf2dbc405a87bfb7827a0f4cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "609eb746a3e34155af75ef3f5dee2578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Scenarios:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "772bb41c122f4318b9752e84c97b9001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Samples:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated 8 synthetic QA pairs\n",
      "\n",
      "Dataset columns: ['user_input', 'reference_contexts', 'reference', 'synthesizer_name']\n",
      "Dataset shape: (8, 4)\n",
      "\n",
      "Sample questions from the dataset:\n",
      "1. Wht is the significnce of the Academic Year in financial aid?\n",
      "2. What 34 CFR 668.3(a) say about academic year minimums?\n",
      "3. Can you explain what is the criteria for including clinical work in a education program standard term?\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic dataset using abstracted SDG\n",
    "from ragas.testset import TestsetGenerator\n",
    "\n",
    "print(\"Generating synthetic test dataset using RAGAS...\")\n",
    "\n",
    "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
    "\n",
    "# Use subset for cost efficiency\n",
    "# Try PDF docs first as they tend to work better with RAGAS\n",
    "testset_docs = pdf_docs[:15] + loan_complaint_data[:10]  # Mixed approach\n",
    "\n",
    "golden_dataset = generator.generate_with_langchain_docs(\n",
    "    testset_docs, \n",
    "    testset_size=8\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Generated {len(golden_dataset)} synthetic QA pairs\")\n",
    "\n",
    "# Convert to pandas for easier viewing\n",
    "df = golden_dataset.to_pandas()\n",
    "print(f\"\\nDataset columns: {list(df.columns)}\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Show sample questions\n",
    "print(\"\\nSample questions from the dataset:\")\n",
    "if 'question' in df.columns:\n",
    "    question_col = 'question'\n",
    "elif 'user_input' in df.columns:\n",
    "    question_col = 'user_input'\n",
    "elif len(df.columns) > 0:\n",
    "    question_col = df.columns[0]  # Use first column as fallback\n",
    "    print(f\"Using column '{question_col}' as questions:\")\n",
    "else:\n",
    "    print(\"No columns found in dataset!\")\n",
    "    question_col = None\n",
    "\n",
    "if question_col:\n",
    "    for i in range(min(3, len(df))):\n",
    "        print(f\"{i+1}. {df.iloc[i][question_col]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.5: Create LangSmith Dataset (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating LangSmith dataset...\n",
      "‚úÖ Created LangSmith dataset: Retriever-Evaluation-20250727_190006\n",
      "üìä Added 8 examples to dataset\n"
     ]
    }
   ],
   "source": [
    "# Create LangSmith dataset for advanced evaluation (if LangSmith is available)\n",
    "try:\n",
    "    from langsmith import Client\n",
    "    \n",
    "    if os.environ.get(\"LANGCHAIN_API_KEY\"):\n",
    "        print(\"Creating LangSmith dataset...\")\n",
    "        \n",
    "        client = Client()\n",
    "        dataset_name = f\"Retriever-Evaluation-{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        \n",
    "        # Create dataset\n",
    "        langsmith_dataset = client.create_dataset(\n",
    "            dataset_name=dataset_name,\n",
    "            description=\"Synthetic data for retriever evaluation using RAGAS\"\n",
    "        )\n",
    "        \n",
    "        # Add examples to LangSmith dataset\n",
    "        df = golden_dataset.to_pandas()\n",
    "        \n",
    "        # Find the correct column names\n",
    "        if 'question' in df.columns:\n",
    "            question_col = 'question'\n",
    "        elif 'user_input' in df.columns:\n",
    "            question_col = 'user_input'\n",
    "        else:\n",
    "            question_col = df.columns[0]\n",
    "            \n",
    "        if 'answer' in df.columns:\n",
    "            answer_col = 'answer'\n",
    "        elif 'reference' in df.columns:\n",
    "            answer_col = 'reference'\n",
    "        else:\n",
    "            answer_col = df.columns[1] if len(df.columns) > 1 else question_col\n",
    "        \n",
    "        # Add examples\n",
    "        for idx, row in df.iterrows():\n",
    "            client.create_example(\n",
    "                inputs={\n",
    "                    \"question\": row[question_col]\n",
    "                },\n",
    "                outputs={\n",
    "                    \"answer\": row[answer_col] if answer_col != question_col else \"Generated answer\"\n",
    "                },\n",
    "                metadata={\n",
    "                    \"source\": \"ragas_synthetic\",\n",
    "                    \"retriever_evaluation\": True\n",
    "                },\n",
    "                dataset_id=langsmith_dataset.id\n",
    "            )\n",
    "        \n",
    "        print(f\"‚úÖ Created LangSmith dataset: {dataset_name}\")\n",
    "        print(f\"üìä Added {len(df)} examples to dataset\")\n",
    "        \n",
    "        # Store for later use\n",
    "        LANGSMITH_DATASET_NAME = dataset_name\n",
    "        USE_LANGSMITH = True\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  LangSmith API key not found, skipping dataset creation\")\n",
    "        USE_LANGSMITH = False\n",
    "        LANGSMITH_DATASET_NAME = None\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  LangSmith not available, install with: pip install langsmith\")\n",
    "    USE_LANGSMITH = False\n",
    "    LANGSMITH_DATASET_NAME = None\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  LangSmith setup failed: {e}\")\n",
    "    USE_LANGSMITH = False\n",
    "    LANGSMITH_DATASET_NAME = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set Up Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up retrievers...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.retrievers import ParentDocumentRetriever, EnsembleRetriever\n",
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain_cohere import CohereRerank\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient, models\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "\n",
    "# Initialize models\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "chat_model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "print(\"Setting up retrievers...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 1. Naive retriever ready\n",
      "‚úÖ 2. BM25 retriever ready\n",
      "‚úÖ 3. Multi-query retriever ready\n",
      "‚úÖ 4. Parent document retriever ready\n",
      "‚úÖ 5. Contextual compression retriever ready\n",
      "‚úÖ 6. Ensemble retriever ready\n",
      "\n",
      "‚úÖ All retrievers initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# 1. Naive Retriever\n",
    "vectorstore = Qdrant.from_documents(\n",
    "    all_docs,\n",
    "    embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"LoanComplaints\"\n",
    ")\n",
    "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "print(\"‚úÖ 1. Naive retriever ready\")\n",
    "\n",
    "# 2. BM25 Retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(all_docs)\n",
    "bm25_retriever.k = 5\n",
    "print(\"‚úÖ 2. BM25 retriever ready\")\n",
    "\n",
    "# 3. Multi-Query Retriever\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=naive_retriever, llm=chat_model\n",
    ")\n",
    "print(\"‚úÖ 3. Multi-query retriever ready\")\n",
    "\n",
    "# 4. Parent Document Retriever\n",
    "parent_docs = all_docs\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=750)\n",
    "\n",
    "# Create new QdrantClient and collection\n",
    "client = QdrantClient(location=\":memory:\")\n",
    "client.create_collection(\n",
    "    collection_name=\"full_documents\",\n",
    "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
    ")\n",
    "\n",
    "parent_document_vectorstore = QdrantVectorStore(\n",
    "    collection_name=\"full_documents\", \n",
    "    embedding=embeddings, \n",
    "    client=client\n",
    ")\n",
    "\n",
    "store = InMemoryStore()\n",
    "parent_document_retriever = ParentDocumentRetriever(\n",
    "    vectorstore=parent_document_vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    ")\n",
    "\n",
    "parent_document_retriever.add_documents(parent_docs, ids=None)\n",
    "print(\"‚úÖ 4. Parent document retriever ready\")\n",
    "\n",
    "# 5. Contextual Compression Retriever\n",
    "compressor = CohereRerank(model=\"rerank-v3.5\")\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=naive_retriever\n",
    ")\n",
    "print(\"‚úÖ 5. Contextual compression retriever ready\")\n",
    "\n",
    "# 6. Ensemble Retriever\n",
    "retriever_list = [bm25_retriever, naive_retriever, parent_document_retriever, compression_retriever, multi_query_retriever]\n",
    "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=retriever_list, weights=equal_weighting\n",
    ")\n",
    "print(\"‚úÖ 6. Ensemble retriever ready\")\n",
    "\n",
    "print(\"\\n‚úÖ All retrievers initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Evaluation functions ready\n"
     ]
    }
   ],
   "source": [
    "def evaluate_retriever_simple(retriever, retriever_name, questions):\n",
    "    \"\"\"\n",
    "    Simple evaluation function that measures retrieval performance\n",
    "    \"\"\"\n",
    "    print(f\"\\nEvaluating {retriever_name}...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    total_docs_retrieved = 0\n",
    "    successful_retrievals = 0\n",
    "    \n",
    "    for i, question in enumerate(questions):\n",
    "        try:\n",
    "            # Retrieve documents\n",
    "            docs = retriever.get_relevant_documents(question)\n",
    "            total_docs_retrieved += len(docs)\n",
    "            successful_retrievals += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error on question {i+1}: {e}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    avg_docs_per_query = total_docs_retrieved / len(questions) if questions else 0\n",
    "    success_rate = successful_retrievals / len(questions) if questions else 0\n",
    "    latency = end_time - start_time\n",
    "    \n",
    "    results = {\n",
    "        'retriever_name': retriever_name,\n",
    "        'success_rate': success_rate,\n",
    "        'avg_docs_per_query': avg_docs_per_query,\n",
    "        'total_latency': latency,\n",
    "        'avg_latency_per_query': latency / len(questions) if questions else 0\n",
    "    }\n",
    "    \n",
    "    print(f\"  ‚úÖ Success rate: {success_rate:.2%}\")\n",
    "    print(f\"  ‚úÖ Avg docs per query: {avg_docs_per_query:.1f}\")\n",
    "    print(f\"  ‚úÖ Latency: {latency:.2f}s\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def estimate_cost(retriever_name, num_queries):\n",
    "    \"\"\"Estimate API costs per retriever type\"\"\"\n",
    "    cost_per_query = {\n",
    "        'Naive': 0.002,  # OpenAI embedding calls\n",
    "        'BM25': 0.0,     # No API calls\n",
    "        'Multi-Query': 0.008,  # Multiple LLM calls + embeddings\n",
    "        'Parent Document': 0.003,  # Embeddings + some overhead\n",
    "        'Contextual Compression': 0.015,  # Cohere rerank + embeddings\n",
    "        'Ensemble': 0.020,  # All of the above combined\n",
    "    }\n",
    "    return cost_per_query.get(retriever_name.split()[0], 0.005) * num_queries\n",
    "\n",
    "print(\"‚úÖ Evaluation functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Run Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on 8 questions...\n",
      "============================================================\n",
      "\n",
      "Evaluating Naive...\n",
      "  ‚úÖ Success rate: 100.00%\n",
      "  ‚úÖ Avg docs per query: 5.0\n",
      "  ‚úÖ Latency: 2.66s\n",
      "\n",
      "Evaluating BM25...\n",
      "  ‚úÖ Success rate: 100.00%\n",
      "  ‚úÖ Avg docs per query: 5.0\n",
      "  ‚úÖ Latency: 0.04s\n",
      "\n",
      "Evaluating Multi-Query...\n",
      "  ‚úÖ Success rate: 100.00%\n",
      "  ‚úÖ Avg docs per query: 7.4\n",
      "  ‚úÖ Latency: 17.18s\n",
      "\n",
      "Evaluating Parent Document...\n",
      "  ‚úÖ Success rate: 100.00%\n",
      "  ‚úÖ Avg docs per query: 2.8\n",
      "  ‚úÖ Latency: 2.14s\n",
      "\n",
      "Evaluating Contextual Compression...\n",
      "  ‚úÖ Success rate: 100.00%\n",
      "  ‚úÖ Avg docs per query: 3.0\n",
      "  ‚úÖ Latency: 3.67s\n",
      "\n",
      "Evaluating Ensemble...\n",
      "  ‚úÖ Success rate: 100.00%\n",
      "  ‚úÖ Avg docs per query: 11.5\n",
      "  ‚úÖ Latency: 25.76s\n",
      "\n",
      "‚úÖ All evaluations completed!\n"
     ]
    }
   ],
   "source": [
    "# Extract questions from RAGAS dataset\n",
    "df = golden_dataset.to_pandas()\n",
    "\n",
    "# Find the correct question column\n",
    "if 'question' in df.columns:\n",
    "    question_col = 'question'\n",
    "elif 'user_input' in df.columns:\n",
    "    question_col = 'user_input'\n",
    "elif len(df.columns) > 0:\n",
    "    question_col = df.columns[0]  # Use first column as fallback\n",
    "    print(f\"Using column '{question_col}' as questions\")\n",
    "else:\n",
    "    raise ValueError(\"No suitable question column found in RAGAS dataset!\")\n",
    "\n",
    "questions = df[question_col].tolist()\n",
    "\n",
    "print(f\"Running evaluation on {len(questions)} questions...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define retrievers to evaluate\n",
    "retrievers_to_test = [\n",
    "    (naive_retriever, \"Naive\"),\n",
    "    (bm25_retriever, \"BM25\"),\n",
    "    (multi_query_retriever, \"Multi-Query\"),\n",
    "    (parent_document_retriever, \"Parent Document\"),\n",
    "    (compression_retriever, \"Contextual Compression\"),\n",
    "    (ensemble_retriever, \"Ensemble\")\n",
    "]\n",
    "\n",
    "# Run evaluations\n",
    "results = []\n",
    "for retriever, name in retrievers_to_test:\n",
    "    result = evaluate_retriever_simple(retriever, name, questions)\n",
    "    result['estimated_cost'] = estimate_cost(name, len(questions))\n",
    "    results.append(result)\n",
    "\n",
    "print(\"\\n‚úÖ All evaluations completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Performance Summary:\n",
      "        retriever_name  success_rate  avg_docs_per_query  total_latency  estimated_cost\n",
      "                 Naive           1.0               5.000         2.6639           0.016\n",
      "                  BM25           1.0               5.000         0.0387           0.000\n",
      "           Multi-Query           1.0               7.375        17.1759           0.064\n",
      "       Parent Document           1.0               2.750         2.1368           0.040\n",
      "Contextual Compression           1.0               3.000         3.6677           0.040\n",
      "              Ensemble           1.0              11.500        25.7602           0.160\n",
      "\n",
      "üèÜ WINNERS:\n",
      "‚ö° Fastest: BM25 (0.04s)\n",
      "üí∞ Cheapest: BM25 ($0.0000)\n",
      "üìö Most Comprehensive: Ensemble (11.5 docs/query)\n",
      "üéñÔ∏è  Best Overall: BM25 (300.689)\n"
     ]
    }
   ],
   "source": [
    "# Create results dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Filter successful retrievers\n",
    "successful_results = results_df[results_df['success_rate'] > 0.8].copy()\n",
    "\n",
    "if len(successful_results) == 0:\n",
    "    print(\"‚ö†Ô∏è  No retrievers achieved >80% success rate. Showing all results:\")\n",
    "    successful_results = results_df.copy()\n",
    "\n",
    "# Display main metrics\n",
    "display_cols = ['retriever_name', 'success_rate', 'avg_docs_per_query', \n",
    "               'total_latency', 'estimated_cost']\n",
    "print(\"\\nüìà Performance Summary:\")\n",
    "print(successful_results[display_cols].round(4).to_string(index=False))\n",
    "\n",
    "# Find best performers\n",
    "fastest = successful_results.loc[successful_results['total_latency'].idxmin()]\n",
    "cheapest = successful_results.loc[successful_results['estimated_cost'].idxmin()]\n",
    "most_docs = successful_results.loc[successful_results['avg_docs_per_query'].idxmax()]\n",
    "\n",
    "# Calculate combined score (simple weighted average)\n",
    "successful_results = successful_results.copy()\n",
    "successful_results['combined_score'] = (\n",
    "    0.4 * successful_results['success_rate'] + \n",
    "    0.3 * (1 / (successful_results['total_latency'] + 1)) + \n",
    "    0.3 * (1 / (successful_results['estimated_cost'] + 0.001))\n",
    ")\n",
    "\n",
    "best_overall = successful_results.loc[successful_results['combined_score'].idxmax()]\n",
    "\n",
    "print(\"\\nüèÜ WINNERS:\")\n",
    "print(f\"‚ö° Fastest: {fastest['retriever_name']} ({fastest['total_latency']:.2f}s)\")\n",
    "print(f\"üí∞ Cheapest: {cheapest['retriever_name']} (${cheapest['estimated_cost']:.4f})\")\n",
    "print(f\"üìö Most Comprehensive: {most_docs['retriever_name']} ({most_docs['avg_docs_per_query']:.1f} docs/query)\")\n",
    "print(f\"üéñÔ∏è  Best Overall: {best_overall['retriever_name']} ({best_overall['combined_score']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Final Analysis and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí° RECOMMENDATIONS BY USE CASE:\n",
      "\n",
      "1. ‚ö° For Speed: BM25\n",
      "   - Fastest response time: 0.04s\n",
      "   - Good for: Real-time applications, high-throughput systems\n",
      "\n",
      "2. üí∞ For Cost Efficiency: BM25\n",
      "   - Lowest cost: $0.0000\n",
      "   - Good for: Budget-conscious deployments, high-volume usage\n",
      "\n",
      "3. üìö For Comprehensive Results: Ensemble\n",
      "   - Most documents per query: 11.5\n",
      "   - Good for: Research applications, thorough analysis\n",
      "\n",
      "4. ‚öñÔ∏è  For Balanced Performance: BM25\n",
      "   - Best combined score: 300.689\n",
      "   - Good for: General-purpose applications, balanced requirements\n",
      "\n",
      "üîç KEY INSIGHTS:\n",
      "- RAGAS provides realistic test questions based on actual data\n",
      "- BM25 is typically fastest and cheapest (no API calls)\n",
      "- Embedding-based methods provide better semantic understanding\n",
      "- Multi-query retrieval improves recall but increases cost\n",
      "- Ensemble methods balance different strengths\n",
      "- Compression/reranking improves quality but adds latency\n",
      "- Parent-document retrievers provide more context per result\n",
      "\n",
      "üìà EVALUATION METRICS:\n",
      "- Success Rate: Percentage of queries processed successfully\n",
      "- Docs Per Query: Average number of documents retrieved\n",
      "- Latency: Time to retrieve and process documents\n",
      "- Cost: Estimated API usage costs\n",
      "\n",
      "üìä EVALUATION COMPLETED: 2025-07-27 19:01:54\n"
     ]
    }
   ],
   "source": [
    "if len(successful_results) > 0:\n",
    "    print(\"\\nüí° RECOMMENDATIONS BY USE CASE:\")\n",
    "    print(f\"\\n1. ‚ö° For Speed: {fastest['retriever_name']}\")\n",
    "    print(f\"   - Fastest response time: {fastest['total_latency']:.2f}s\")\n",
    "    print(f\"   - Good for: Real-time applications, high-throughput systems\")\n",
    "    \n",
    "    print(f\"\\n2. üí∞ For Cost Efficiency: {cheapest['retriever_name']}\")\n",
    "    print(f\"   - Lowest cost: ${cheapest['estimated_cost']:.4f}\")\n",
    "    print(f\"   - Good for: Budget-conscious deployments, high-volume usage\")\n",
    "    \n",
    "    print(f\"\\n3. üìö For Comprehensive Results: {most_docs['retriever_name']}\")\n",
    "    print(f\"   - Most documents per query: {most_docs['avg_docs_per_query']:.1f}\")\n",
    "    print(f\"   - Good for: Research applications, thorough analysis\")\n",
    "    \n",
    "    print(f\"\\n4. ‚öñÔ∏è  For Balanced Performance: {best_overall['retriever_name']}\")\n",
    "    print(f\"   - Best combined score: {best_overall['combined_score']:.3f}\")\n",
    "    print(f\"   - Good for: General-purpose applications, balanced requirements\")\n",
    "    \n",
    "    print(\"\\nüîç KEY INSIGHTS:\")\n",
    "    print(\"- RAGAS provides realistic test questions based on actual data\")\n",
    "    print(\"- BM25 is typically fastest and cheapest (no API calls)\")\n",
    "    print(\"- Embedding-based methods provide better semantic understanding\")\n",
    "    print(\"- Multi-query retrieval improves recall but increases cost\")\n",
    "    print(\"- Ensemble methods balance different strengths\")\n",
    "    print(\"- Compression/reranking improves quality but adds latency\")\n",
    "    print(\"- Parent-document retrievers provide more context per result\")\n",
    "    \n",
    "    print(\"\\nüìà EVALUATION METRICS:\")\n",
    "    print(\"- Success Rate: Percentage of queries processed successfully\")\n",
    "    print(\"- Docs Per Query: Average number of documents retrieved\")\n",
    "    print(\"- Latency: Time to retrieve and process documents\")\n",
    "    print(\"- Cost: Estimated API usage costs\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  All retrievers had issues. Check your setup and data.\")\n",
    "\n",
    "print(f\"\\nüìä EVALUATION COMPLETED: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: LangSmith Advanced Evaluation for ALL Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî¨ Running LangSmith evaluation for ALL retrievers...\n",
      "üìä Evaluating 6 retrievers with LangSmith...\n",
      "üîç Evaluators: QA Accuracy, Helpfulness, Empathy\n",
      "\n",
      "üîç Evaluating Naive retriever...\n",
      "View the evaluation results for experiment: 'retriever_naive-58d23808' at:\n",
      "https://smith.langchain.com/o/52c4cf8c-3e10-4738-ae6a-bc186d787252/datasets/81c8a79c-b026-4d96-a5b9-08dfd82ac30f/compare?selectedSessions=9de82c1f-a6cc-4a97-8522-ec662ed34081\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f005cbd872a04b18b4a72d2b89efebbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Naive evaluation completed successfully\n",
      "\n",
      "üîç Evaluating BM25 retriever...\n",
      "View the evaluation results for experiment: 'retriever_bm25-5f0e18c1' at:\n",
      "https://smith.langchain.com/o/52c4cf8c-3e10-4738-ae6a-bc186d787252/datasets/81c8a79c-b026-4d96-a5b9-08dfd82ac30f/compare?selectedSessions=aecc04e8-75db-40ff-bbe4-2af347aaa0a6\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69892bc4858340248261a80f65b6219e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ BM25 evaluation completed successfully\n",
      "\n",
      "üîç Evaluating Multi-Query retriever...\n",
      "View the evaluation results for experiment: 'retriever_multi_query-4e8be06f' at:\n",
      "https://smith.langchain.com/o/52c4cf8c-3e10-4738-ae6a-bc186d787252/datasets/81c8a79c-b026-4d96-a5b9-08dfd82ac30f/compare?selectedSessions=60c18c72-cda8-402d-af1b-a661538a9f90\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "584ec25a6b2f48df8744d8334dc53b2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Multi-Query evaluation completed successfully\n",
      "\n",
      "üîç Evaluating Parent-Document retriever...\n",
      "View the evaluation results for experiment: 'retriever_parent_document-b1d87665' at:\n",
      "https://smith.langchain.com/o/52c4cf8c-3e10-4738-ae6a-bc186d787252/datasets/81c8a79c-b026-4d96-a5b9-08dfd82ac30f/compare?selectedSessions=cfb819df-fec9-4089-960e-e55faa75ad1c\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b4444016a3b4a6980c9d70299953aed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Parent-Document evaluation completed successfully\n",
      "\n",
      "üîç Evaluating Contextual-Compression retriever...\n",
      "View the evaluation results for experiment: 'retriever_contextual_compression-370372a6' at:\n",
      "https://smith.langchain.com/o/52c4cf8c-3e10-4738-ae6a-bc186d787252/datasets/81c8a79c-b026-4d96-a5b9-08dfd82ac30f/compare?selectedSessions=39c2a652-01bf-4a40-8655-73c2889fa947\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c7311d2f40a490a9886253db998a1c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Contextual-Compression evaluation completed successfully\n",
      "\n",
      "üîç Evaluating Ensemble retriever...\n",
      "View the evaluation results for experiment: 'retriever_ensemble-c22124af' at:\n",
      "https://smith.langchain.com/o/52c4cf8c-3e10-4738-ae6a-bc186d787252/datasets/81c8a79c-b026-4d96-a5b9-08dfd82ac30f/compare?selectedSessions=a4759aad-d30e-444e-bad3-9e365561c572\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "431a9f3921cd4df28468a4f0305261fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ensemble evaluation completed successfully\n",
      "\n",
      "üéØ All retriever evaluations completed!\n",
      "üìä Check LangSmith dashboard for detailed comparison results!\n",
      "üîç Each retriever has been evaluated for: QA Accuracy, Helpfulness, Empathy\n"
     ]
    }
   ],
   "source": [
    "if USE_LANGSMITH:\n",
    "    print(\"\\nüî¨ Running LangSmith evaluation for ALL retrievers...\")\n",
    "    \n",
    "    try:\n",
    "        from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
    "        from langchain.prompts import ChatPromptTemplate\n",
    "        from langchain.schema import StrOutputParser\n",
    "        from operator import itemgetter\n",
    "        \n",
    "        # Create RAG chain for evaluation\n",
    "        RAG_PROMPT = \"\"\"Given the provided context and question, answer the question based only on the context.\n",
    "If you cannot answer based on the context, say \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\"\"\"\n",
    "        \n",
    "        rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)\n",
    "        eval_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "        \n",
    "        # QA evaluator (following example.ipynb pattern)\n",
    "        qa_evaluator = LangChainStringEvaluator(\"qa\", config={\"llm\": eval_llm})\n",
    "        \n",
    "        # Labeled helpfulness evaluator (following example.ipynb pattern)\n",
    "        labeled_helpfulness_evaluator = LangChainStringEvaluator(\n",
    "            \"labeled_criteria\",\n",
    "            config={\n",
    "                \"criteria\": {\n",
    "                    \"helpfulness\": (\n",
    "                        \"Is this submission helpful to the user,\"\n",
    "                        \" taking into account the correct reference answer?\"\n",
    "                    )\n",
    "                },\n",
    "                \"llm\": eval_llm\n",
    "            },\n",
    "            prepare_data=lambda run, example: {\n",
    "                \"prediction\": run.outputs[\"output\"],\n",
    "                \"reference\": example.outputs[\"answer\"],\n",
    "                \"input\": example.inputs[\"question\"],\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Empathy evaluator (following example.ipynb pattern)\n",
    "        empathy_evaluator = LangChainStringEvaluator(\n",
    "            \"criteria\",\n",
    "            config={\n",
    "                \"criteria\": {\n",
    "                    \"empathy\": \"Is this response empathetic? Does it make the user feel like they are being heard?\",\n",
    "                },\n",
    "                \"llm\": eval_llm\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Define all retrievers to evaluate\n",
    "        all_retrievers_to_evaluate = [\n",
    "            (naive_retriever, \"Naive\"),\n",
    "            (bm25_retriever, \"BM25\"),\n",
    "            (multi_query_retriever, \"Multi-Query\"),\n",
    "            (parent_document_retriever, \"Parent-Document\"),\n",
    "            (compression_retriever, \"Contextual-Compression\"),\n",
    "            (ensemble_retriever, \"Ensemble\")\n",
    "        ]\n",
    "        \n",
    "        print(f\"üìä Evaluating {len(all_retrievers_to_evaluate)} retrievers with LangSmith...\")\n",
    "        print(\"üîç Evaluators: QA Accuracy, Helpfulness, Empathy\")\n",
    "        \n",
    "        # Evaluate each retriever\n",
    "        for retriever, name in all_retrievers_to_evaluate:\n",
    "            print(f\"\\nüîç Evaluating {name} retriever...\")\n",
    "            \n",
    "            try:\n",
    "                # Create RAG chain for this retriever\n",
    "                rag_chain = (\n",
    "                    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "                    | rag_prompt | eval_llm | StrOutputParser()\n",
    "                )\n",
    "                \n",
    "                # Run evaluation for this retriever\n",
    "                experiment_results = evaluate(\n",
    "                    rag_chain.invoke,\n",
    "                    data=LANGSMITH_DATASET_NAME,\n",
    "                    evaluators=[\n",
    "                        qa_evaluator,\n",
    "                        labeled_helpfulness_evaluator,\n",
    "                        empathy_evaluator\n",
    "                    ],\n",
    "                    metadata={\n",
    "                        \"retriever_type\": name, \n",
    "                        \"evaluation_run\": \"all_retrievers\",\n",
    "                        \"evaluators\": \"qa_helpfulness_empathy\"\n",
    "                    },\n",
    "                    experiment_prefix=f\"retriever_{name.lower().replace(' ', '_').replace('-', '_')}\"\n",
    "                )\n",
    "                \n",
    "                print(f\"‚úÖ {name} evaluation completed successfully\")\n",
    "                \n",
    "                # Add rate limiting delay between retrievers\n",
    "                time.sleep(3)  # 3 second delay between retrievers\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå {name} evaluation failed: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(\"\\nüéØ All retriever evaluations completed!\")\n",
    "        print(\"üìä Check LangSmith dashboard for detailed comparison results!\")\n",
    "        print(\"üîç Each retriever has been evaluated for: QA Accuracy, Helpfulness, Empathy\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå LangSmith evaluation failed: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Skipping LangSmith evaluation (not configured)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever Evaluation: Final Analysis\n",
    "\n",
    "### Overview\n",
    "\n",
    "We evaluated 6 retrieval methods on Federal Student Aid regulatory content using 8 RAGAS-generated questions. The evaluation measured both performance metrics and answer quality through LangSmith evaluation.\n",
    "\n",
    "### Performance Results\n",
    "\n",
    "| Retriever | Success Rate | Docs/Query | Latency (s) | When to Use |\n",
    "|-----------|--------------|------------|-------------|-------------|\n",
    "| BM25 | 100% | 5.0 | 0.04 | Speed + cost efficiency |\n",
    "| Naive | 100% | 5.0 | 2.66 | Semantic understanding |\n",
    "| Parent Document | 100% | 2.8 | 2.14 | Rich context needed |\n",
    "| Contextual Compression | 100% | 3.0 | 3.67 | Quality over quantity |\n",
    "| Multi-Query | 100% | 7.4 | 17.18 | Comprehensive research |\n",
    "| Ensemble | 100% | 11.5 | 25.76 | Maximum coverage |\n",
    "\n",
    "### Quality Assessment\n",
    "\n",
    "The LangSmith evaluation shows all retrievers generated high-quality, accurate answers for complex regulatory questions about academic year requirements, clinical work criteria, and Title IV compliance. The answers demonstrate strong understanding of CFR citations and regulatory nuances.\n",
    "\n",
    "### When Each Retriever Makes Sense\n",
    "\n",
    "**BM25 Retriever** shines when you need blazing fast responses at zero cost. It's perfect for production environments with high query volume, especially when dealing with structured regulatory content that contains specific terminology and citations. The 0.04 second response time makes it ideal for real-time student support systems where instant answers matter most.\n",
    "\n",
    "**Naive Retriever** works best when your users ask questions in natural language rather than using exact regulatory terms. It provides solid semantic understanding while maintaining reasonable speed and cost. This approach handles conversational queries well and serves as an excellent general-purpose solution when you need flexibility across different types of content.\n",
    "\n",
    "**Parent Document Retriever** excels when context is king. It provides fewer documents but each one contains complete, rich information rather than fragments. This method works particularly well for applications where users need to understand the full policy context around their question, making it valuable for compliance officers and detailed policy research.\n",
    "\n",
    "**Contextual Compression Retriever** delivers precision through intelligent reranking. While it takes a bit longer to process, it ensures the most relevant results rise to the top. This approach makes sense when answer quality is more important than speed, particularly for applications where users prefer fewer, highly relevant results over many potentially tangential ones.\n",
    "\n",
    "**Multi-Query Retriever** serves researchers and complex analysis scenarios exceptionally well. It automatically generates multiple query variations to ensure comprehensive coverage, retrieving 7.4 documents per query on average. The higher latency and cost are worthwhile when thoroughness matters more than speed, such as for policy analysis or detailed regulatory research.\n",
    "\n",
    "**Ensemble Retriever** combines the strengths of all methods to provide maximum coverage with 11.5 documents per query. It's the Swiss Army knife of retrieval, best suited for applications where missing relevant information isn't an option. The higher resource requirements are justified when comprehensive coverage is mission-critical.\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "The evaluation revealed that regulatory content works exceptionally well with keyword-based approaches due to its structured nature and specific terminology. However, the choice between methods depends heavily on your specific use case: real-time support systems favor speed, research applications favor coverage, and precision-focused tools benefit from reranking.\n",
    "\n",
    "All methods produced high-quality answers for Federal Student Aid questions, suggesting that the choice between them should focus on operational requirements like speed, cost, and coverage rather than answer quality alone.\n",
    "\n",
    "\n",
    "![LangSmith Screenshot](./img/ls.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
